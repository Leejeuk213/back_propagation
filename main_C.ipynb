{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 헤더파일 선언\n",
    "\n",
    "계산에 필요한 모든 함수들을 미리 모듈화해서 메인 코드의 가독성을 올린다.  \n",
    "\n",
    "추후 수정 및 추가에도 용이하다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "#ifndef FUNCTIONS_H\n",
    "#define FUNCTIONS_H\n",
    "\n",
    "void zeros(double** array, int rows, int cols);\n",
    "double randn();\n",
    "void randnArray(double** array, int rows, int cols);\n",
    "void randArray(double** array, int rows, int cols);\n",
    "void divide_matrix(double** array, int rows, int cols, double value);\n",
    "void mul_matrix(double** array1, double** array2, int rows, int cols, double value);\n",
    "void add_matrix(double** array, int rows, int cols, double value);\n",
    "void dot_product(double** array, double** array1, double** array2, int a, int b, int c);\n",
    "void sigmoid(double** array1, double** array2, int row, int col);\n",
    "void relu(double** array1, double** array2, int row, int col);\n",
    "void softmax(double** array1, double** array2, int row, int col);\n",
    "void log_matrix(double** array1, double** array2, int row, int col);\n",
    "void elementwise_mul(double** array, double** array1, double** array2, int row, int col);\n",
    "void elementwise_minus(double** array, double** array1, double** array2, int row, int col);\n",
    "void elementwise_add(double** array, double** array1, double** array2, int row, int col);\n",
    "void matrix_minus_from_one(double** array1, double** array2, int row, int col);\n",
    "double sum(double** array, int row, int col);\n",
    "void matrix_transpose(double** result, double** matrix, int row, int col);\n",
    "void relu_back(double** result_array, double** array, int row, int col);\n",
    "int find_max_index(double** array, int col);\n",
    "void printArray(double** array, int rows, int cols);\n",
    "\n",
    "#endif\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 함수 설명  \n",
    "\n",
    "모든 계산 함수는 호출 전 계산 결과 size만큼 동적 할당을 한 다음에 연산 결과를 받아옴  \n",
    "\n",
    "1. zeros  \n",
    "행렬을 생성할 때 행렬의 내부 값들을 모두 0으로 한다.  \n",
    "\n",
    "2. randn  \n",
    "정규 분포 난수를 생성하는 함수이다. 가중치를 초기화할 떄 사용한다.  \n",
    "\n",
    "3. randnArray  \n",
    "위의 randn 함수를 호출해 정규 분포 난수로 이루어진 행렬을 반환  \n",
    "\n",
    "4. divide_matrix  \n",
    "행렬을 요소 별로 주어진 값으로 나눔  \n",
    "\n",
    "5. mul_matrix  \n",
    "행렬을 요소 별로 주어진 값으로 곱함  \n",
    "\n",
    "6. add_matrix  \n",
    "행렬을 요소 별로 주어진 값으로 더함  \n",
    "\n",
    "7. dot_product  \n",
    "a x b * b x c 행렬이 들어오면  내적 연산을 해 => a x c 사이즈의 행렬을 반환  \n",
    "\n",
    "8. sigmoid  \n",
    "output layer에서 시그모이드 활성화 함수 적용  \n",
    "\n",
    "9. relu\n",
    "hidden layer에서 relu 활성화 함수 적용  \n",
    "\n",
    "10. softmax  \n",
    "output layer에서 softmax 활성화 함수 적용\n",
    "class 많을 때는 sigmoid보다 성능이 좋다.  \n",
    "softmax를 사용할 것임  \n",
    "\n",
    "11. log_matrix  \n",
    "행렬의 각 요소에 log를 취해줌  \n",
    "\n",
    "12. elementwise_mul   \n",
    "두 행렬을 받으면 행렬의 요소끼리 곱해서 반환해 줌  \n",
    "\n",
    "13. elementwise_minus  \n",
    "두 행렬을 받으면 행렬의 앞 행렬의 값에서 뒤 행렬의 값을 빼서 반환해 줌    \n",
    "\n",
    "14. elementwise_add  \n",
    "두 행렬을 받으면 행렬의 요소끼리 더해서 반환해 줌  \n",
    "\n",
    "15. matrix_minus_from_one  \n",
    "행렬의 요소를 1에서 빼는 함수 cross entropy loss 계산 시 사용  \n",
    "\n",
    "16. sum  \n",
    "행렬을 받으면 요소끼리 전부 더 해서 반환해 줌  \n",
    "\n",
    "17. matrix_transpose  \n",
    "행렬 전치 함수 (행과 열을 바꿈)  \n",
    "전치된 행렬의 행과 열 수는 원본 행렬의 열과 행 수와 같음  \n",
    "\n",
    "18. relu_back  \n",
    "역전파 단계에서 relu의 편미분 값을 구함  \n",
    "\n",
    "19. find_max_index  \n",
    "one hot encoding된 class에서 가장 큰 값이 예측한 정답값임  \n",
    "가장 큰 값이 무엇인지 반환함  \n",
    "\n",
    "20. printArray  \n",
    "테스트용으로 행렬을 출력하기 위한 함수  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "#include<stdio.h>\n",
    "#include <stdlib.h>\n",
    "#include <string.h>\n",
    "\n",
    "#define _USE_MATH_DEFINES   \n",
    "#include <math.h>\n",
    "\n",
    "#include \"functions.h\"\n",
    "\n",
    "모든 요소를 0으로 초기화\n",
    "void zeros(double** array, int rows, int cols) {\n",
    "    for (int i = 0; i < rows; i++) {\n",
    "        for (int j = 0; j < cols; j++) {\n",
    "            array[i][j] = 0;\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "정규 분포 난수 생성 함수\n",
    "double randn() {\n",
    "    double u1 = (double)rand() / RAND_MAX; // 0에서 1 사이의 난수 생성\n",
    "    double u2 = (double)rand() / RAND_MAX; // 0에서 1 사이의 또 다른 난수 생성\n",
    "    double z = sqrt(-2 * log(u1)) * cos(2 * M_PI * u2); // 정규 분포로 변환\n",
    "\n",
    "    return z;\n",
    "}\n",
    "\n",
    "정규 분포 난수 행렬 생성 함수\n",
    "void randnArray(double** array, int rows, int cols) {\n",
    "    for (int i = 0; i < rows; i++) {\n",
    "        for (int j = 0; j < cols; j++) {\n",
    "            array[i][j] = randn(); // 정규 분포 난수로 초기화\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "0에서 1 사이의 난수 행렬 생성 함수\n",
    "void randArray(double** array, int rows, int cols) {\n",
    "    for (int i = 0; i < rows; i++) {\n",
    "        for (int j = 0; j < cols; j++) {\n",
    "            array[i][j] = (double)rand() / RAND_MAX;\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "행렬을 주어진 값으로 나누는 함수\n",
    "void divide_matrix(double** array, int rows, int cols, double value) {\n",
    "    for (int i = 0; i < rows; i++) {\n",
    "        for (int j = 0; j < cols; j++) {\n",
    "            array[i][j] = array[i][j] / value;\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "행렬을 주어진 값으로 곱하는 함수\n",
    "void mul_matrix(double** array1, double** array2, int rows, int cols, double value) {\n",
    "    for (int i = 0; i < rows; i++) {\n",
    "        for (int j = 0; j < cols; j++) {\n",
    "            array1[i][j] = array2[i][j] * value;\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "행렬을 주어진 값으로 더하는 함수\n",
    "void add_matrix(double** array, int rows, int cols, double value) {\n",
    "    for (int i = 0; i < rows; i++) {\n",
    "        for (int j = 0; j < cols; j++) {\n",
    "            array[i][j] = array[i][j] + value;\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "a x b * b x c 행렬이 들어오면  내적 연산을 해 => a x c 사이즈의 행렬을 반환\n",
    "void dot_product(double** array, double** array1, double** array2, int a, int b, int c) {\n",
    "\n",
    "    for (int i = 0; i < a; i++) {\n",
    "        for (int j = 0; j < c; j++) {\n",
    "            double product = 0;\n",
    "            for (int k = 0; k < b; k++) {\n",
    "                product += array1[i][k] * array2[k][j];\n",
    "            }\n",
    "            array[i][j] = product;\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "sigmoid 함수\n",
    "void sigmoid(double** array1, double** array2, int row, int col) {\n",
    "    for (int i = 0; i < row; i++) {\n",
    "        for (int j = 0; j < col; j++) {\n",
    "            array1[i][j] = 1 / (1 + exp(-1 * array2[i][j]));\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "relu 함수\n",
    "void relu(double** array1, double** array2, int row, int col) {\n",
    "    for (int i = 0; i < row; i++) {\n",
    "        for (int j = 0; j < col; j++) {\n",
    "            if (array2[i][j] > 0)  array1[i][j] = array2[i][j];\n",
    "            else array1[i][j] = 0;\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "softmax 함수\n",
    "void softmax(double** array1, double** array2, int row, int col) {\n",
    "\n",
    "    double max_value = array2[0][find_max_index(array2, col)];\n",
    "    double sum = 0;\n",
    "    for (int i = 0; i < row; i++) {\n",
    "        for (int j = 0; j < col; j++) {\n",
    "            sum += exp(array2[i][j] - max_value);\n",
    "        }\n",
    "    }\n",
    "    for (int i = 0; i < row; i++) {\n",
    "        for (int j = 0; j < col; j++) {\n",
    "            array1[i][j] = exp(array2[i][j] - max_value) / sum;\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "행렬의 각 요소에 log 취하는 함수\n",
    "void log_matrix(double** array1, double** array2, int row, int col) {\n",
    "    for (int i = 0; i < row; i++) {\n",
    "        for (int j = 0; j < col; j++) {\n",
    "            array1[i][j] = log(array2[i][j]);\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "행렬의 요소끼리 곱하는 함수\n",
    "void elementwise_mul(double** array, double** array1, double** array2, int row, int col) {\n",
    "    for (int i = 0; i < row; i++) {\n",
    "        for (int j = 0; j < col; j++) {\n",
    "            array[i][j] = array1[i][j] * array2[i][j];\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "행렬의 요소끼리 더하는 함수\n",
    "void elementwise_add(double** array, double** array1, double** array2, int row, int col) {\n",
    "\n",
    "    for (int i = 0; i < row; i++) {\n",
    "        for (int j = 0; j < col; j++) {\n",
    "            array[i][j] = array1[i][j] + array2[i][j];\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "행렬의 요소끼리 빼는 함수\n",
    "void elementwise_minus(double** array, double** array1, double** array2, int row, int col) {\n",
    "\n",
    "    for (int i = 0; i < row; i++) {\n",
    "        for (int j = 0; j < col; j++) {\n",
    "            array[i][j] = array1[i][j] - array2[i][j];\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "행렬의 요소를 1에서 빼는 함수\n",
    "void matrix_minus_from_one(double** array1, double** array2, int row, int col) {\n",
    "    for (int i = 0; i < row; i++) {\n",
    "        for (int j = 0; j < col; j++) {\n",
    "            array1[i][j] = 1 - array2[i][j];\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "행렬의 모든 요소의 합을 구하는 함수\n",
    "double sum(double** array, int row, int col) {\n",
    "\n",
    "    double result = 0;\n",
    "    for (int i = 0; i < row; i++) {\n",
    "        for (int j = 0; j < col; j++) {\n",
    "            result += array[i][j];\n",
    "        }\n",
    "    }\n",
    "    free(array);\n",
    "    return result;\n",
    "}\n",
    "\n",
    "행렬 전치 함수 (행과 열을 바꿈)\n",
    "전치된 행렬의 행과 열 수는 원본 행렬의 열과 행 수와 같음\n",
    "void matrix_transpose(double** result, double** matrix, int row, int col) {\n",
    "    \n",
    "    for (int i = 0; i < col; i++) {\n",
    "        for (int j = 0; j < row; j++) {\n",
    "            result[i][j] = matrix[j][i];\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "행렬의 열에서 가장 큰 값의 인덱스를 구하는 함수\n",
    "int find_max_index(double** array, int col) {\n",
    "    int max_index = 0;\n",
    "    double max_value = array[0][0];\n",
    "\n",
    "    for (int i = 1; i < col; i++) {\n",
    "        if (array[0][i] > max_value) {\n",
    "            max_index = i;\n",
    "            max_value = array[0][i];\n",
    "        }\n",
    "    }\n",
    "    return max_index;\n",
    "}\n",
    "\n",
    "relu 함수의 역전파 함수\n",
    "void relu_back(double** result_array, double** array, int row, int col) {\n",
    "    for (int i = 0; i < row; i++) {\n",
    "        for (int j = 0; j < col; j++) {\n",
    "            if (array[i][j] > 0)  result_array[i][j] = 1;\n",
    "            else result_array[i][j] = 0;\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "배열 출력 함수 (테스트용)\n",
    "void printArray(double** array, int rows, int cols) {\n",
    "    for (int i = 0; i < rows; i++) {\n",
    "        for (int j = 0; j < cols; j++) {\n",
    "            printf(\"%lf \", array[i][j]);\n",
    "        }\n",
    "        printf(\"\\n\");\n",
    "    }\n",
    "    printf(\"\\n\");\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 헤더파일 호출 및 전역변수(Hyper parameter, 신경망 내부 변수) 선언"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#include <string.h>\n",
    "#include <time.h>\n",
    "#include <math.h>\n",
    "#include \"functions.h\" \n",
    "\n",
    "double** w2;\n",
    "double** b2;\n",
    "double** w3;\n",
    "double** b3;\n",
    "double** w4;\n",
    "double** b4;\n",
    "double** a1;\n",
    "double** z1;\n",
    "double** a2;\n",
    "double** z2;\n",
    "double** a3;\n",
    "double** z3;\n",
    "double** a4;\n",
    "double** z4;\n",
    "\n",
    "\n",
    "hyper parameter 설정\n",
    "int input_nodes = 256; // 16 x 16\n",
    "int hidden_nodes = 96;\n",
    "int second_hidden_nodes = 48;\n",
    "\n",
    "int output_nodes = 7; // t, u, v, x, y, z\n",
    "double learning_rate = 0.002;\n",
    "int epochs = 175;\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# forward propagation 구현  \n",
    "\n",
    "1. input layer  \n",
    "(1, input_nodes)a1, z1 = input data\n",
    "\n",
    "2. first hidden layer  \n",
    "w2(input_nodes, hidden_nodes) -> 내적으로 결과 shape (1, hidden_nodes)로 변경  \n",
    "편향을 더해줌  \n",
    "(1, hidden_nodes)z2 = a1 * w2 + b2   \n",
    "(1, hidden_nodes)a2 = relu(z2)  \n",
    "\n",
    "3. second hidden layer  \n",
    "w3(hidden_nodes, second_hidden_nodes) -> 내적으로 결과 shape (1, second_hidden_nodes)로 변경  \n",
    "편향을 더해줌  \n",
    "(1, second_hidden_nodes)z3 = a2 * w3 + b3  \n",
    "(1, second_hidden_nodes)a3 = relu(z3)  \n",
    "\n",
    "4. output layer  \n",
    "w3(second_hidden_nodes, output_nodes) -> 내적으로 결과 shape (1, output_nodes)로 변경  \n",
    "편향을 더해줌  \n",
    "(1, output_nodes)z4 = a3 * w4 + b4  \n",
    "(1, output_nodes)a4 = softmax(z4) \n",
    "\n",
    "5. loss 계산   \n",
    "\n",
    "나온 예측 a4와 정답 값 target_data를 이용해 cross entropy loss 적용  \n",
    "\n",
    "6. loss 반환  \n",
    "계산된 loss 반환  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "double forward(double** input_data, double** target_data) {\n",
    "    double delta = 1e-7; // log 무한대 발산 방지\n",
    "\n",
    "    cross entropy loss 결과 담을 변수들 동적 할당\n",
    "    double** result1 = (double**)malloc(1 * sizeof(double*));\n",
    "    result1[0] = (double*)malloc(output_nodes * sizeof(double));\n",
    "\n",
    "    double** result2 = (double**)malloc(1 * sizeof(double*));\n",
    "    result2[0] = (double*)malloc(output_nodes * sizeof(double));\n",
    "\n",
    "    double** result = (double**)malloc(1 * sizeof(double*));\n",
    "    result[0] = (double*)malloc(output_nodes * sizeof(double));\n",
    "\n",
    "    // 처음 input data를 z1, a1에 넣어줌\n",
    "    for (int i = 0; i < input_nodes; i++) {\n",
    "        z1[0][i] = input_data[0][i];\n",
    "        a1[0][i] = input_data[0][i];\n",
    "    }\n",
    "\n",
    "    // forward propagation\n",
    "\n",
    "    // z2 = a1 * w2 + b2\n",
    "    // a2 = relu(z2) \n",
    "    dot_product(z2, a1, w2, 1, input_nodes, hidden_nodes);\n",
    "    elementwise_add(z2, z2, b2, 1, hidden_nodes);\n",
    "    relu(a2, z2, 1, hidden_nodes);\n",
    "\n",
    "    // z3 = a2 * w3 + b3\n",
    "    // a3 = relu(z3)\n",
    "    dot_product(z3, a2, w3, 1, hidden_nodes, second_hidden_nodes);\n",
    "    elementwise_add(z3, z3, b3, 1, second_hidden_nodes);\n",
    "    relu(a3, z3, 1, second_hidden_nodes);\n",
    "\n",
    "    // z4 = a3 * w4 + b4\n",
    "    // a4 = softmax(z4)\n",
    "    dot_product(z4, a3, w4, 1, second_hidden_nodes, output_nodes);\n",
    "    elementwise_add(z4, z4, b4, 1, output_nodes);\n",
    "    softmax(a4, z4, 1, output_nodes);\n",
    "\n",
    "    // cross entropy loss \n",
    "    double** log_result = (double**)malloc(1 * sizeof(double*));\n",
    "    log_result[0] = (double*)malloc(output_nodes * sizeof(double));\n",
    "\n",
    "    // a4 + delta 발산 방지\n",
    "    add_matrix(a4, 1, output_nodes, delta);\n",
    "    // log(a4) = log_result\n",
    "    log_matrix(log_result, a4, 1, output_nodes);\n",
    "    // target_data * log(a4) = target_data * log_result = result1\n",
    "    elementwise_mul(result1, target_data, log_result, 1, output_nodes);\n",
    "\n",
    "    double** a = (double**)malloc(1 * sizeof(double*));\n",
    "    a[0] = (double*)malloc(output_nodes * sizeof(double));\n",
    "    double** b = (double**)malloc(1 * sizeof(double*));\n",
    "    b[0] = (double*)malloc(output_nodes * sizeof(double));\n",
    "\n",
    "    // 1 - target_data a에 넣어줌\n",
    "    matrix_minus_from_one(a, target_data, 1, output_nodes);\n",
    "    // 1 - a4 b에 넣어줌\n",
    "    matrix_minus_from_one(b, a4, 1, output_nodes);\n",
    "    // 1 - a4 + delta 발산 방지\n",
    "    add_matrix(b, 1, output_nodes, delta);\n",
    "    // log(1 - a4)\n",
    "    log_matrix(b, b, 1, output_nodes);\n",
    "    // (1 - target_data) * log(1 - a4) = a * b = result2\n",
    "    elementwise_mul(result2, a, b, 1, output_nodes);\n",
    "    // result = target_data * log(a4) + (1 - target_data) * log(1 - a4) = result1 + result2\n",
    "    elementwise_add(result, result1, result2, 1, output_nodes);\n",
    "\n",
    "    free(result1);\n",
    "    free(result2);\n",
    "    free(log_result);\n",
    "    free(a);\n",
    "    free(b);\n",
    "\n",
    "    // loss 반환\n",
    "    return -1 * sum(result, 1, output_nodes);\n",
    "}\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# backward propagation 구현  \n",
    "\n",
    "1. forward 함수 호출해 우선 forward 진행  \n",
    "\n",
    "2. output layer 계층 역전파  \n",
    "loss4 = dl/dz4를 구함  \n",
    "loss4 = (a4 - target_data) = dl/da4 * da4/dz4 = dl/dz4  \n",
    "z4 = w4 * a3 + b4 -> dz4/dw4 = a3  \n",
    "연산을 위해 a3 transpose    \n",
    "\n",
    "w4 업데이트  \n",
    "dw4 = a3_T * loss4  = dl/dz4 * dz4/dw4 = dl/dw4  \n",
    "dw4 = dl/dw4 * learning_rate  \n",
    "w4 = w4 - dw4  \n",
    "\n",
    "b4 업데이트  \n",
    "db4 = dl/dz4 * dz4/db4 = dl/db4, dl/db4 = 1  \n",
    "db4 = dl/db4 * learning_rate \n",
    "b4 = b4 - db4  \n",
    "\n",
    "3. second hidden layer 계층 역전파  \n",
    "loss3 = dl/dz3를 구함  \n",
    "z4 = w4 * a3 + b4 -> dz4/da3 = w4  \n",
    "계산을 위해 w4 transpose  \n",
    "loss3 = dl/dz4 * dz4/da3 = dl/da3 * da3/dz3= loss4 * w4_T * relu_back(a3)  \n",
    "\n",
    "z3 = w3 * a2 + b3 -> dz3/dw3 = a2  \n",
    "연산을 위해 a2 transpose   \n",
    "\n",
    "w3 업데이트  \n",
    "dw3 = a2_T * loss3  = dl/dz3 * dz3/dw3 = dl/dw3  \n",
    "dw3 = dl/dw3 * learning_rate  \n",
    "w3 = w3 - dw3  \n",
    "\n",
    "b3 업데이트  \n",
    "db3 = dl/dz3 * dz3/db3 = dl/db3, dl/db3 = 1  \n",
    "db3 = dl/db3 * learning_rate \n",
    "b3 = b3 - db3  \n",
    "\n",
    "4. hidden layer 계층 역전파  \n",
    "loss2 = dl/dz2를 구함  \n",
    "z3 = w3 * a2 + b3 -> dz3/da2 = w3  \n",
    "계산을 위해 w3 transpose  \n",
    "loss2 = dl/dz3 * dz3/da2 = dl/da2 * da2/dz2 = loss3 * w3_T * relu_back(a2)  \n",
    "\n",
    "z2 = w2 * a1 + b2 -> dz2/dw2 = a1  \n",
    "연산을 위해 a1 transpose    \n",
    "\n",
    "w2 업데이트  \n",
    "dw2 = a2_T * loss2  = dl/dz2 * dz2/dw2 = dl/dw2  \n",
    "dw2 = dl/dw2 * learning_rate  \n",
    "w2 = w2 - dw2  \n",
    "\n",
    "b2 업데이트  \n",
    "db2 = dl/dz2 * dz2/db2 = dl/db2, dl/db2 = 1  \n",
    "db2 = dl/db2 * learning_rate \n",
    "b2 = b2 - db2  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "double train(double** input_data, double** target_data) {\n",
    "\n",
    "    // forward propagation\n",
    "    double loss_result = forward(input_data, target_data);\n",
    "\n",
    "    double** loss4 = (double**)malloc(1 * sizeof(double*));\n",
    "    loss4[0] = (double*)malloc(output_nodes * sizeof(double));\n",
    "    double** loss3 = (double**)malloc(1 * sizeof(double*));\n",
    "    loss3[0] = (double*)malloc(second_hidden_nodes * sizeof(double));\n",
    "    double** loss2 = (double**)malloc(1 * sizeof(double*));\n",
    "    loss2[0] = (double*)malloc(hidden_nodes * sizeof(double));\n",
    "\n",
    "    // back propagation\n",
    "    //  loss4 = (a4 - target_data) = dl/da4 * da4/dz4 = dl/dz4\n",
    "    elementwise_minus(loss4, a4, target_data, 1, output_nodes);\n",
    "\n",
    "    // 미리 연산 결과를 담을 변수들 동적 할당\n",
    "    double** a_T = (double**)malloc(second_hidden_nodes * sizeof(double*));\n",
    "    for (int i = 0; i < second_hidden_nodes; i++) a_T[i] = (double*)malloc(1 * sizeof(double));\n",
    "\n",
    "    double** dw = (double**)malloc(second_hidden_nodes * sizeof(double*));\n",
    "    for (int i = 0; i < second_hidden_nodes; i++) dw[i] = (double*)malloc(output_nodes * sizeof(double));\n",
    "\n",
    "    double** db = (double**)malloc(1 * sizeof(double*));\n",
    "    db[0] = (double*)malloc(output_nodes * sizeof(double));\n",
    "\n",
    "    // z4 = w4 * a3 + b4 -> dz4/dw4 = a3 \n",
    "    // 연산을 위해 a3 transpose\n",
    "    matrix_transpose(a_T, a3, 1, second_hidden_nodes);\n",
    "    // dw4 = a3_T * loss4 = dl/dz4 * dz4/dw4 = dl/dw4\n",
    "    dot_product(dw, a_T, loss4, second_hidden_nodes, 1, output_nodes);\n",
    "    // dw4 = dl/dw4 * learning_rate \n",
    "    mul_matrix(dw, dw, second_hidden_nodes, output_nodes, learning_rate);\n",
    "    // w4 = w4 - dw4\n",
    "    elementwise_minus(w4, w4, dw, second_hidden_nodes, output_nodes);\n",
    "\n",
    "    // db4 = dl/dz4 * dz4/db4 = dl/db4, dl/db4 = 1\n",
    "    // db4 = dl/db4 * learning_rate\n",
    "    mul_matrix(db, loss4, 1, output_nodes, learning_rate);\n",
    "    // b4 = b4 - db4\n",
    "    elementwise_minus(b4, b4, db, 1, output_nodes);\n",
    "\n",
    "    free(a_T);\n",
    "    free(dw);\n",
    "    free(db);\n",
    "\n",
    "    double** w_T = (double**)malloc(output_nodes * sizeof(double*));\n",
    "    for (int i = 0; i < output_nodes; i++) w_T[i] = (double*)malloc(second_hidden_nodes * sizeof(double));\n",
    "\n",
    "    double** da = (double**)malloc(1 * sizeof(double*));\n",
    "    da[0] = (double*)malloc(second_hidden_nodes * sizeof(double));\n",
    "\n",
    "    double** dz = (double**)malloc(1 * sizeof(double*));\n",
    "    dz[0] = (double*)malloc(second_hidden_nodes * sizeof(double));\n",
    "\n",
    "    // z4 = w4 * a3 + b4 -> dz4/da3 = w4\n",
    "    // 계산을 위해 w4 transpose\n",
    "    matrix_transpose(w_T, w4, second_hidden_nodes, output_nodes);\n",
    "    // dl/dz4 * dz4/da3 = dl/da3 = loss4 * w4_T\n",
    "    dot_product(da, loss4, w_T, 1, output_nodes, second_hidden_nodes);\n",
    "\n",
    "    // relu_back = da3/dz3 \n",
    "    relu_back(dz, a3, 1, second_hidden_nodes);\n",
    "\n",
    "    // loss3 = dl/da3 * da3/dz3 = dl/dz3 = loss4 * w4_T * relu_back(a3)\n",
    "    elementwise_mul(loss3, da, dz, 1, second_hidden_nodes);\n",
    "\n",
    "\n",
    "    a_T = (double**)malloc(hidden_nodes * sizeof(double*));\n",
    "    for (int i = 0; i < hidden_nodes; i++) a_T[i] = (double*)malloc(1 * sizeof(double));\n",
    "\n",
    "    dw = (double**)malloc(hidden_nodes * sizeof(double*));\n",
    "    for (int i = 0; i < hidden_nodes; i++) dw[i] = (double*)malloc(second_hidden_nodes * sizeof(double));\n",
    "\n",
    "    db = (double**)malloc(1 * sizeof(double*));\n",
    "    db[0] = (double*)malloc(second_hidden_nodes * sizeof(double));\n",
    "\n",
    "    // z3 = w3 * a2 + b3 -> dz3/dw3 = a2\n",
    "    // 계산을 위해 a2 transpose\n",
    "    matrix_transpose(a_T, a2, 1, hidden_nodes);\n",
    "    // dw3 = a2_T * loss3 = dl/dz3 * dz3/dw3 = dl/dw3\n",
    "    dot_product(dw, a_T, loss3, hidden_nodes, 1, second_hidden_nodes);\n",
    "    // dw3 = dl/dw3 * learning_rate\n",
    "    mul_matrix(dw, dw, hidden_nodes, second_hidden_nodes, learning_rate);\n",
    "    // w3 = w3 - dw3\n",
    "    elementwise_minus(w3, w3, dw, hidden_nodes, second_hidden_nodes);\n",
    "\n",
    "    // db3 = dl/dz3 * dz3/db3 = dl/db3, dl/db3 = 1\n",
    "    // db3 = dl/db3 * learning_rate\n",
    "    mul_matrix(db, loss3, 1, second_hidden_nodes, learning_rate);\n",
    "    // b3 = b3 - db3\n",
    "    elementwise_minus(b3, b3, db, 1, second_hidden_nodes);\n",
    "\n",
    "    free(w_T);\n",
    "    free(da);\n",
    "    free(dz);\n",
    "    free(a_T);\n",
    "    free(dw);\n",
    "    free(db);\n",
    "\n",
    "    w_T = (double**)malloc(second_hidden_nodes * sizeof(double*));\n",
    "    for (int i = 0; i < second_hidden_nodes; i++) w_T[i] = (double*)malloc(hidden_nodes * sizeof(double));\n",
    "\n",
    "    da = (double**)malloc(1 * sizeof(double*));\n",
    "    da[0] = (double*)malloc(hidden_nodes * sizeof(double));\n",
    "\n",
    "    dz = (double**)malloc(1 * sizeof(double*));\n",
    "    dz[0] = (double*)malloc(hidden_nodes * sizeof(double));\n",
    "\n",
    "    // z3 = w3 * a2 + b3 -> dz3/da2 = w3\n",
    "    // 계산을 위해 w3 transpose\n",
    "    matrix_transpose(w_T, w3, hidden_nodes, second_hidden_nodes);\n",
    "    // dl/dz3 * dz3/da2 = dl/da2 = = loss3 * w3_T\n",
    "    dot_product(da, loss3, w_T, 1, second_hidden_nodes, hidden_nodes);\n",
    "    // relu_back = da2/dz2\n",
    "    relu_back(dz, a2, 1, hidden_nodes);\n",
    "    // loss2 = dl/da2 * da2/dz2 = dl/dz2 = loss3 * w3_T * relu_back(a2)\n",
    "    elementwise_mul(loss2, da, dz, 1, hidden_nodes);\n",
    "\n",
    "    a_T = (double**)malloc(input_nodes * sizeof(double*));\n",
    "    for (int i = 0; i < input_nodes; i++) a_T[i] = (double*)malloc(1 * sizeof(double));\n",
    "\n",
    "    dw = (double**)malloc(input_nodes * sizeof(double*));\n",
    "    for (int i = 0; i < input_nodes; i++) dw[i] = (double*)malloc(hidden_nodes * sizeof(double));\n",
    "\n",
    "    db = (double**)malloc(1 * sizeof(double*));\n",
    "    db[0] = (double*)malloc(hidden_nodes * sizeof(double));\n",
    "\n",
    "    // z2 = w2 * a1 + b2 -> dz2/dw2 = a1\n",
    "    // 계산을 위해 a1 transpose\n",
    "    matrix_transpose(a_T, a1, 1, input_nodes);\n",
    "    // dw2 = a2_T * loss2 = dl/dz2 * dz2/dw2 = dl/dw2\n",
    "    dot_product(dw, a_T, loss2, input_nodes, 1, hidden_nodes);\n",
    "    // dw2 = dl/dw2 * learning_rate\n",
    "    mul_matrix(dw, dw, input_nodes, hidden_nodes, learning_rate);\n",
    "    // w2 = w2 - dw2\n",
    "    elementwise_minus(w2, w2, dw, input_nodes, hidden_nodes);\n",
    "\n",
    "    // db2 = dl/dz2 * dz2/db2 = dl/db2, dl/db2 = 1\n",
    "    // db2 = dl/db2 * learning_rate\n",
    "    mul_matrix(db, loss2, 1, hidden_nodes, learning_rate);\n",
    "    // b2 = b2 - db2\n",
    "    elementwise_minus(b2, b2, db, 1, hidden_nodes);\n",
    "\n",
    "    free(w_T);\n",
    "    free(da);\n",
    "    free(dz);\n",
    "    free(a_T);\n",
    "    free(dw);\n",
    "    free(db);\n",
    "\n",
    "    free(loss2);\n",
    "    free(loss3);\n",
    "    free(loss4);\n",
    "\n",
    "    return loss_result;\n",
    "}\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict 함수  \n",
    "\n",
    "테스트 데이터를 하나 받아와 학습된 가중치로 forward 진행한 뒤  \n",
    "output layer 결과 값 중 가장 큰 값의 index를 반환함  \n",
    "가장 큰 값은 one hot encoding에서 예측한 정답을 의미함  \n",
    "{0:t, 1:u, 2:v, 3:w, 4:x, 5:y, 6:z}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "int predict(double** input_data) {\n",
    "\n",
    "    double** Z2;\n",
    "    double** A2;\n",
    "    double** Z3;\n",
    "    double** A3;\n",
    "    double** Z4;\n",
    "    double** A4;\n",
    "\n",
    "    Z2 = (double**)malloc(1 * sizeof(double*));\n",
    "    Z2[0] = (double*)malloc(hidden_nodes * sizeof(double));\n",
    "    zeros(Z2, 1, hidden_nodes);\n",
    "\n",
    "    A2 = (double**)malloc(1 * sizeof(double*));\n",
    "    A2[0] = (double*)malloc(hidden_nodes * sizeof(double));\n",
    "    zeros(A2, 1, hidden_nodes);\n",
    "\n",
    "    Z3 = (double**)malloc(1 * sizeof(double*));\n",
    "    Z3[0] = (double*)malloc(second_hidden_nodes * sizeof(double));\n",
    "    zeros(Z3, 1, second_hidden_nodes);\n",
    "\n",
    "    A3 = (double**)malloc(1 * sizeof(double*));\n",
    "    A3[0] = (double*)malloc(second_hidden_nodes * sizeof(double));\n",
    "    zeros(A3, 1, second_hidden_nodes);\n",
    "\n",
    "    Z4 = (double**)malloc(1 * sizeof(double*));\n",
    "    Z4[0] = (double*)malloc(output_nodes * sizeof(double));\n",
    "    zeros(Z4, 1, output_nodes);\n",
    "\n",
    "    A4 = (double**)malloc(1 * sizeof(double*));\n",
    "    A4[0] = (double*)malloc(output_nodes * sizeof(double));\n",
    "    zeros(A4, 1, output_nodes);\n",
    "\n",
    "    // 기본적인 구조 forward와 동일 \n",
    "    dot_product(Z2, input_data, w2, 1, input_nodes, hidden_nodes);\n",
    "    elementwise_add(Z2, Z2, b2, 1, hidden_nodes);\n",
    "    relu(A2, Z2, 1, hidden_nodes);\n",
    "\n",
    "    dot_product(Z3, A2, w3, 1, hidden_nodes, second_hidden_nodes);\n",
    "    elementwise_add(Z3, Z3, b3, 1, second_hidden_nodes);\n",
    "    relu(A3, Z3, 1, second_hidden_nodes);\n",
    "\n",
    "    dot_product(Z4, A3, w4, 1, second_hidden_nodes, output_nodes);\n",
    "    elementwise_add(Z4, Z4, b4, 1, output_nodes);\n",
    "    softmax(A4, Z4, 1, output_nodes);\n",
    "\n",
    "    // 출력값 중 가장 큰 값의 인덱스를 리턴\n",
    "    int predicted_num = find_max_index(A4, output_nodes);\n",
    "\n",
    "    free(Z2);\n",
    "    free(A2);\n",
    "    free(Z3);\n",
    "    free(A3);\n",
    "    free(Z4);\n",
    "    free(A4);\n",
    "\n",
    "    return predicted_num;\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# accuracy 함수  \n",
    "받아온 test data를 0.01 ~ 1.0의 값을 가지게끔 정규화를 한 뒤에  \n",
    "predict 함수를 통해 예측을 진행함  \n",
    "정답 라벨링 값과 예측한 값이 같으면 cnt를 1 더 해주고  \n",
    "모든 test data에 대해 같은 과정을 진행한 뒤 총 정확도를 출력함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "void accuracy(double input_data[140][257]) {\n",
    "\n",
    "    double count = 0;\n",
    "    double** data = (double**)malloc(1 * sizeof(double*));\n",
    "    data[0] = (double*)malloc(input_nodes * sizeof(double));\n",
    "\n",
    "    // test data 140개를 predict 함수를 통해 예측한 값과 label이 같으면 count를 증가시킴\n",
    "    for (int i = 0; i < 140; i++) {\n",
    "        int label = input_data[i][256];\n",
    "        for (int j = 0; j < 256; j++) {\n",
    "            data[0][j] = (input_data[i][j] / 255.0 * 0.99) + 0.01;\n",
    "        }\n",
    "        int predicted_num = predict(data);\n",
    "\n",
    "        if (label == predicted_num) count++;\n",
    "    }\n",
    "    printf(\"count %lf, Current Accuracy = %lf\\n\", count, 100 * (count / 140));\n",
    "    free(data);\n",
    "}\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# init 함수  \n",
    "계산에 사용할 변수들 미리 동적할당 함  \n",
    "\n",
    "w2(input_nodes, hidden_nodes)  \n",
    "w3(hidden_nodes, second_hidden_nodes)  \n",
    "w4(second_hidden_nodes, output_nodes)  \n",
    "\n",
    "b2(1, hidden_nodes)  \n",
    "b3(1, second_hidden_nodes)  \n",
    "b4(1, output_nodes)\n",
    "\n",
    "z1(1,input_nodes)  \n",
    "a1(1,input_nodes)  \n",
    "\n",
    "z2(1,hidden_nodes)    \n",
    "a2(1,hidden_nodes)  \n",
    "\n",
    "z3(1,second_hidden_nodes)  \n",
    "a3(1,second_hidden_nodes)  \n",
    "\n",
    "z4(1,output_nodes)    \n",
    "a4(1,output_nodes)  \n",
    "\n",
    "가중치의 경우 he initalization을 통해 초기화를 함  \n",
    "\n",
    "정규 분포의 난수로 이루어진 것을 입력에 제곱근을 씌워준 것으로 나누는 xavier inialization에 2를 곱해준 형태임  \n",
    "\n",
    "편향은 0~1 난수로 만듬  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "void init()\n",
    "{\n",
    "    // input layer, hidden layer, output layer 선형회귀 값, 출력 값 선언 및 가중치 설정 \n",
    "    w2 = (double**)malloc(input_nodes * sizeof(double*));\n",
    "    for (int i = 0; i < input_nodes; i++) w2[i] = (double*)malloc(hidden_nodes * sizeof(double));\n",
    "    randnArray(w2, input_nodes, hidden_nodes);\n",
    "    divide_matrix(w2, input_nodes, hidden_nodes, sqrt(input_nodes / 2)); // he 초기화\n",
    "\n",
    "    b2 = (double**)malloc(1 * sizeof(double*));\n",
    "    b2[0] = (double*)malloc(hidden_nodes * sizeof(double));\n",
    "    randArray(b2, 1, hidden_nodes);\n",
    "\n",
    "    w3 = (double**)malloc(hidden_nodes * sizeof(double*));\n",
    "    for (int i = 0; i < hidden_nodes; i++) w3[i] = (double*)malloc(second_hidden_nodes * sizeof(double));\n",
    "    randnArray(w3, hidden_nodes, second_hidden_nodes);\n",
    "    divide_matrix(w3, hidden_nodes, second_hidden_nodes, sqrt(hidden_nodes / 2));\n",
    "\n",
    "    b3 = (double**)malloc(1 * sizeof(double*));\n",
    "    b3[0] = (double*)malloc(second_hidden_nodes * sizeof(double));\n",
    "    randArray(b3, 1, second_hidden_nodes);\n",
    "\n",
    "    w4 = (double**)malloc(second_hidden_nodes * sizeof(double*));\n",
    "    for (int i = 0; i < second_hidden_nodes; i++) w4[i] = (double*)malloc(output_nodes * sizeof(double));\n",
    "    randnArray(w4, second_hidden_nodes, output_nodes);\n",
    "    divide_matrix(w4, second_hidden_nodes, output_nodes, sqrt(second_hidden_nodes / 2));\n",
    "\n",
    "    b4 = (double**)malloc(1 * sizeof(double*));\n",
    "    b4[0] = (double*)malloc(output_nodes * sizeof(double));\n",
    "    randArray(b4, 1, output_nodes);\n",
    "\n",
    "    // z, a 동적 할당 및 초기화\n",
    "    z1 = (double**)malloc(1 * sizeof(double*));\n",
    "    z1[0] = (double*)malloc(input_nodes * sizeof(double));\n",
    "    zeros(z1, 1, input_nodes);\n",
    "\n",
    "    a1 = (double**)malloc(1 * sizeof(double*));\n",
    "    a1[0] = (double*)malloc(input_nodes * sizeof(double));\n",
    "    zeros(a1, 1, input_nodes);\n",
    "\n",
    "    z2 = (double**)malloc(1 * sizeof(double*));\n",
    "    z2[0] = (double*)malloc(hidden_nodes * sizeof(double));\n",
    "    zeros(z2, 1, hidden_nodes);\n",
    "\n",
    "    a2 = (double**)malloc(1 * sizeof(double*));\n",
    "    a2[0] = (double*)malloc(hidden_nodes * sizeof(double));\n",
    "    zeros(a2, 1, hidden_nodes);\n",
    "\n",
    "    z3 = (double**)malloc(1 * sizeof(double*));\n",
    "    z3[0] = (double*)malloc(second_hidden_nodes * sizeof(double));\n",
    "    zeros(z3, 1, second_hidden_nodes);\n",
    "\n",
    "    a3 = (double**)malloc(1 * sizeof(double*));\n",
    "    a3[0] = (double*)malloc(second_hidden_nodes * sizeof(double));\n",
    "    zeros(a3, 1, second_hidden_nodes);\n",
    "\n",
    "    z4 = (double**)malloc(1 * sizeof(double*));\n",
    "    z4[0] = (double*)malloc(output_nodes * sizeof(double));\n",
    "    zeros(z4, 1, output_nodes);\n",
    "\n",
    "    a4 = (double**)malloc(1 * sizeof(double*));\n",
    "    a4[0] = (double*)malloc(output_nodes * sizeof(double));\n",
    "    zeros(a4, 1, output_nodes);\n",
    "}\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# main 함수  \n",
    "\n",
    "train data 파일 315개 test data 파일 140개를 받아옴  \n",
    "라벨을 기준으로 class마다 one hot encoding을 해준 뒤  \n",
    "정규화된 학습 데이터로 epochs만큼 학습을 함  \n",
    "50번마다 loss를 출력하고 학습이 끝날 때마다 정확도를 출력함  \n",
    "마지막에 hyper parameter들을 출력함 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "int main() {\n",
    "\n",
    "    // 시드설정 가중치를 위한\n",
    "    srand(time(NULL));\n",
    "\n",
    "    // test파일 train 파일 불러오기\n",
    "    FILE* file;\n",
    "    char buffer[1024];\n",
    "    int i, j;\n",
    "    double test_data[140][257];\n",
    "    double train_data[315][257];\n",
    "\n",
    "    file = fopen(\"test.csv\", \"r\");\n",
    "    i = 0;\n",
    "    while (fgets(buffer, sizeof(buffer), file) != NULL) {\n",
    "        char* token = strtok(buffer, \",\");\n",
    "        j = 0;\n",
    "        while (token != NULL) {\n",
    "            test_data[i][j] = atoi(token);\n",
    "            token = strtok(NULL, \",\");\n",
    "            j++;\n",
    "        }\n",
    "        i++;\n",
    "    }\n",
    "    fclose(file);\n",
    "\n",
    "    file = fopen(\"train.csv\", \"r\");\n",
    "    i = 0;\n",
    "    while (fgets(buffer, sizeof(buffer), file) != NULL) {\n",
    "        char* token = strtok(buffer, \",\");\n",
    "        j = 0;\n",
    "        while (token != NULL) {\n",
    "            train_data[i][j] = atoi(token);\n",
    "            token = strtok(NULL, \",\");\n",
    "            j++;\n",
    "        }\n",
    "        i++;\n",
    "    }\n",
    "    // 파일 닫기\n",
    "    fclose(file);\n",
    "\n",
    "    init();\n",
    "\n",
    "    for (int i = 0; i < epochs; i++) {\n",
    "\n",
    "        for (int step = 0; step < 315; step++) {\n",
    "\n",
    "            // target data, input data 동적 할당 및 초기화\n",
    "            double** target_data = (double**)malloc(1 * sizeof(double*));\n",
    "            target_data[0] = (double*)malloc(output_nodes * sizeof(double));\n",
    "            zeros(target_data, 1, output_nodes);\n",
    "            // 0.01로 초기화\n",
    "            add_matrix(target_data, 1, output_nodes, 0.01);\n",
    "\n",
    "            double** input_data = (double**)malloc(1 * sizeof(double*));\n",
    "            input_data[0] = (double*)malloc(input_nodes * sizeof(double));\n",
    "            int label = train_data[step][256];\n",
    "            // one hot encoding \n",
    "            // 정답 레이블에 해당하는 인덱스에 0.99를 넣어줌 나머지는 0.01\n",
    "            target_data[0][label] = 0.99;\n",
    "\n",
    "            for (int j = 0; j < 256; j++) {\n",
    "                // 이미지 데이터 0~255 -> 0.01~1.0 \n",
    "                // 정규화\n",
    "                input_data[0][j] = (train_data[step][j] / 255.0 * 0.99) + 0.01;\n",
    "            }\n",
    "            double loss_result = train(input_data, target_data);\n",
    "\n",
    "            // 계산 도중에 z2의 값이 무한대로 갔음 0.00001으로 나눈 것을 0으로 인식해버림 가중치 재설정하고 학습 처음부터 다시 시작함.\n",
    "            // 초기 한번만 CHECK하면 이후에 무한대로 가는 경우는 없었음. 따라서 이런 식으로 하드코딩\n",
    "            for (int i = 0; i < hidden_nodes; i++) {\n",
    "                if (isinf(z2[0][i])) {\n",
    "                    free(w2);\n",
    "                    free(b2);\n",
    "                    free(w3);\n",
    "                    free(b3);\n",
    "                    free(w4);\n",
    "                    free(b4);\n",
    "                    free(a1);\n",
    "                    free(z1);\n",
    "                    free(a2);\n",
    "                    free(z2);\n",
    "                    free(a3);\n",
    "                    free(z3);\n",
    "                    free(a4);\n",
    "                    free(z4);\n",
    "                    init();\n",
    "                    i = 0;\n",
    "                    step = -1;\n",
    "                }\n",
    "            }\n",
    "            // 50번마다 loss 출력\n",
    "            if (step % 50 == 0) {\n",
    "                printf(\"current epochs = %d step = %d, loss_val = %lf\\n\", i, step, loss_result);\n",
    "            }\n",
    "\n",
    "            free(target_data);\n",
    "            free(input_data);\n",
    "        }\n",
    "        // 한번 학습이 끝날 때마다 정확도 출력\n",
    "        accuracy(test_data);\n",
    "    }\n",
    "\n",
    "    free(w2);\n",
    "    free(b2);\n",
    "    free(w3);\n",
    "    free(b3);\n",
    "    free(w4);\n",
    "    free(b4);\n",
    "    free(a1);\n",
    "    free(z1);\n",
    "    free(a2);\n",
    "    free(z2);\n",
    "    free(a3);\n",
    "    free(z3);\n",
    "    free(a4);\n",
    "    free(z4);\n",
    "\n",
    "    \n",
    "    printf(\"input_nodes: %d\\n\", input_nodes);\n",
    "    printf(\"hidden_nodes: %d\\n\", hidden_nodes);\n",
    "    printf(\"second_hidden_nodes: %d\\n\", second_hidden_nodes);\n",
    "    printf(\"output_nodes: %d\\n\", output_nodes);\n",
    "    printf(\"learning_rate: %lf\\n\", learning_rate);\n",
    "    printf(\"epochs: %d\\n\", epochs);\n",
    "    \n",
    "    return 0;\n",
    "}\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![nn](c_result/result1.JPG)\n",
    "\n",
    "초기 설정한 파라미터로 학습한 결과  \n",
    "오버피팅이 된 모습을 확인할 수 있다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![nn](c_result/result2.JPG)  \n",
    "\n",
    "\n",
    "학습횟수를 어떻게 조절하느냐에 따라 결과가 완전히 다른 모습을 확인할 수 있었다.  \n",
    "이 파라미터에서 학습률만 올려서 다시 실행해 보았다.  \n",
    "\n",
    "\n",
    "![nn](c_result/result3.JPG)  \n",
    "\n",
    "이전 결과보다 훨씬 빠르게 오버피팅이 된 것을 볼 수 있었다.  \n",
    "\n",
    "반대로 학습률을 낮춘다음에 다시 학습을 시켜봤다.  \n",
    "\n",
    "\n",
    "![nn](c_result/result4.JPG)  \n",
    "\n",
    "best accruacy는 아니지만 오버피팅을 방지할 순 있었다.  \n",
    "적절한 학습률과 학습횟수를 미리 설정해서 돌린다면  \n",
    "최선의 결과를 얻을 수 있을 것이다. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "초기 하이퍼 파라미터에서  레이어의 수를 줄이고 학습을 진행해보았다.  \n",
    "\n",
    "![nn](c_result/result5.JPG)  \n",
    "\n",
    "오버피팅이 되는 지점을 찾기 위해 학습 횟수를 늘려보았다.  \n",
    "\n",
    "67번째 학습부터 정확도가 감소하기 시작하였고 75에 이르러선 완전히 바닥에 도달한 것을 볼 수 있었다.    \n",
    "\n",
    "\n",
    "![nn](c_result/result6.JPG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "실험의 결과 정상적으로 학습됐을 경우 랜덤 생성된 가중치에 따라 정확도가 91 ~ 95선에서 모델이 완성되는 것을 확인할 수 있었고  \n",
    "오버피팅 지점을 넘어서는 순간 loss값이 매우 커지고 정확도가 바닥에 이르는 것을 확인할 수 있었다.  \n",
    "\n",
    "적절한 하이퍼 파라미터를 찾아 좋게 학습된 결과 \n",
    "\n",
    "-![nn](c_result/result_7.JPG)  \n",
    "\n",
    "\n",
    "매번 저렇게 좋은 결과가 나오진 않지만 최소한 최악의 경우인 정확도가 14 정도가 나오는  \n",
    "오버피팅 지점에 빠지진 않게끔 하이퍼 파라미터를 조정할 수는 있었다.  \n",
    "\n",
    "학습을 빠르게 진행하고 싶다면 학습률을 높이고 학습횟수는 줄이면 되고  \n",
    "반대의 경우도 가능하다. 학습률을 낮추고 학습횟수를 늘리면 천천히 학습을 진행할 수 있다.  \n",
    "\n",
    "레이어의 수 같은 경우   \n",
    "레이어의 수를 작게 하면 학습이 느려 학습횟수를 많이 가져가야 한다는 단점이 있었다.  \n",
    "\n",
    "-![nn](c_result/result9.JPG)  \n",
    "\n",
    "\n",
    "다음과 같이 학습을 100번 했을 때 결과가 좋게 나오는 것을 확인할 수 있었다.    \n",
    " \n",
    "\n",
    "레이어의 수를 많이 키우면 학습 횟수가 많이 필요하진 않지만  \n",
    "한 번의 학습을 할 때 시간이 오래 걸리고 또 파이썬 같은 경우에는 c에 비해 속도가 느려 더 느리게 된다.  \n",
    "\n",
    "레이어의 수를 늘리고 싶을 떄 적절한 하이퍼 파라미터는  \n",
    "first hidden layer는 60 ~ 80  \n",
    "second hidden layer는 30 ~ 40  \n",
    "학습률은 0.001  \n",
    "학습횟수는 40 ~ 50이고  \n",
    "\n",
    "레이어의 수를 낮추고 싶을 때 적절한 하이퍼 파라미터는  \n",
    "레이어를 30 ~ 40 그리고 15 ~ 20으로 내리고  \n",
    "학습률을 올리거나 학습 횟수를 낮추는 것이다.  \n",
    "\n",
    "두 방식 다 중요한 것은 오버피팅이 되는 지점을 찾고 그 지점까지 가지 않게끔 본인이 원하는 하이퍼파라미터로 조정하는 것이다.  \n",
    "\n",
    "레이어의 수를 줄이고 학습률이나 학습횟수를 높이거나  \n",
    "레이어의 수를 높이고 학습률이나 학습횟수를 줄이는 것은  \n",
    "본인의 취향에 달린 것 같다.  \n",
    "\n",
    "오버피팅에 빠지지 않는다면 정확도는 초기 생성되는 가중치 그리고 학습되는 정도에 따라 90 ~ 97에 학습이 완료되는 것을 확인할 수 있었다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
